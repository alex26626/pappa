{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model's predictions against gold labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW:\n",
      "           elin      lena     oscar       agg  model  mean_human\n",
      "elin   1.000000  0.608571  0.617143  0.748571    0.0    0.612857\n",
      "lena   0.608571  1.000000  0.668571  0.817143    0.0    0.638571\n",
      "oscar  0.617143  0.668571  1.000000  0.840000    0.0    0.642857\n",
      "agg    0.748571  0.817143  0.840000  1.000000    0.0    0.801905\n",
      "model  0.000000  0.000000  0.000000  0.000000    1.0    0.000000\n",
      "KAPPA:\n",
      "           elin      lena     oscar       agg     model  mean_human\n",
      "elin   1.000000  0.486754  0.499290  0.672779 -0.000752    0.493022\n",
      "lena   0.486754  1.000000  0.533719  0.750879 -0.001283    0.510237\n",
      "oscar  0.499290  0.533719  1.000000  0.781927 -0.001324    0.516505\n",
      "agg    0.672779  0.750879  0.781927  1.000000 -0.001087    0.735195\n",
      "model -0.000752 -0.001283 -0.001324 -0.001087  1.000000   -0.001120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "# read inference results\n",
    "df_predictions = pd.read_csv('../results/flan-t5-small_01_June_2023_12_11_42.tsv', sep='\\t')\n",
    "\n",
    "# retrieve columns starting with \"gold\" and their \"names\"\n",
    "gold_labels = df_predictions.filter(regex='^gold', axis=1)\n",
    "gold_names = [col.split('gold_')[-1] for col in gold_labels.columns]\n",
    "human_names = [name for name in gold_names if 'agg' not in name]\n",
    "\n",
    "# define tables where to store results\n",
    "df_kappa = pd.DataFrame(columns=gold_names+['model'], index=gold_names+['model']).fillna(1.0)\n",
    "df_accuracy = pd.DataFrame(columns=gold_names+['model'], index=gold_names+['model']).fillna(1.0)\n",
    "\n",
    "\n",
    "for i, col in enumerate(gold_labels.columns):\n",
    "    # compare agreement with gold labels\n",
    "    kappa = cohen_kappa_score(df_predictions['prediction'].astype(str), gold_labels[col].astype(str))\n",
    "    accuracy = accuracy_score(df_predictions['prediction'].astype(str), gold_labels[col].astype(str))\n",
    "    # store results\n",
    "    df_kappa.loc['model', gold_names[i]] = df_kappa.loc[gold_names[i], 'model'] = kappa\n",
    "    df_accuracy.loc['model', gold_names[i]] = df_accuracy.loc[gold_names[i], 'model'] = accuracy\n",
    "\n",
    "    for j, col2 in enumerate(gold_labels.columns):\n",
    "        if i < j:\n",
    "            # compare agreement of gold labels with each other\n",
    "            kappa = cohen_kappa_score(gold_labels[col].astype(str), gold_labels[col2].astype(str))\n",
    "            accuracy = accuracy_score(gold_labels[col].astype(str), gold_labels[col2].astype(str))\n",
    "            # store results\n",
    "            df_kappa.loc[gold_names[i], gold_names[j]] = df_kappa.loc[gold_names[j], gold_names[i]] = kappa\n",
    "            df_accuracy.loc[gold_names[i], gold_names[j]] = df_accuracy.loc[gold_names[j], gold_names[i]] = accuracy\n",
    "\n",
    "# compute average agreement between humans\n",
    "df_kappa['mean_human'] = df_kappa[human_names].mean(axis=1)\n",
    "df_accuracy['mean_human'] = df_accuracy[human_names].mean(axis=1)\n",
    "for name in human_names:\n",
    "    # correct for humans fully agreeing with themselves\n",
    "    df_kappa.mean_human[name] = (df_kappa[human_names].loc[name].sum() - 1.0) / (len(human_names) - 1.0)\n",
    "    df_accuracy.mean_human[name] = (df_accuracy[human_names].loc[name].sum() - 1.0) / (len(human_names) - 1.0)\n",
    "\n",
    "print('RAW:')\n",
    "print(df_accuracy)\n",
    "print('KAPPA:')\n",
    "print(df_kappa)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from Dirk's evaluation of GPT:\n",
    "\n",
    "````python\n",
    "dim1\n",
    "350\n",
    "RAW:\n",
    "               elin      lena     oscar  aggregate       GPT  mean_human\n",
    "elin       1.000000  0.608571  0.617143   0.748571  0.494286    0.612857\n",
    "lena       0.608571  1.000000  0.668571   0.817143  0.545714    0.638571\n",
    "oscar      0.617143  0.668571  1.000000   0.840000  0.528571    0.642857\n",
    "aggregate  0.748571  0.817143  0.840000   1.000000  0.548571    0.801905\n",
    "GPT        0.494286  0.545714  0.528571   0.548571  1.000000    0.522857\n",
    "KAPPA:\n",
    "               elin      lena     oscar  aggregate       GPT  mean_human\n",
    "elin       1.000000  0.486754  0.499290   0.672779  0.354546    0.493022\n",
    "lena       0.486754  1.000000  0.533719   0.750879  0.388609    0.510237\n",
    "oscar      0.499290  0.533719  1.000000   0.781927  0.364316    0.516505\n",
    "aggregate  0.672779  0.750879  0.781927   1.000000  0.406302    0.735195\n",
    "GPT        0.354546  0.388609  0.364316   0.406302  1.000000    0.369157\n",
    "\n",
    "dim2\n",
    "225\n",
    "RAW:\n",
    "               elin      lena     oscar  aggregate       GPT  mean_human\n",
    "elin       1.000000  0.613333  0.631111   0.835556  0.591111    0.622222\n",
    "lena       0.613333  1.000000  0.706667   0.511111  0.760000    0.660000\n",
    "oscar      0.631111  0.706667  1.000000   0.795556  0.577778    0.668889\n",
    "aggregate  0.835556  0.511111  0.795556   1.000000  0.462222    0.714074\n",
    "GPT        0.591111  0.760000  0.577778   0.462222  1.000000    0.642963\n",
    "KAPPA:\n",
    "               elin      lena     oscar  aggregate       GPT  mean_human\n",
    "elin       1.000000  0.014797  0.194175   0.677288  0.110251    0.104486\n",
    "lena       0.014797  1.000000  0.181818   0.081155  0.145570    0.098308\n",
    "oscar      0.194175  0.181818  1.000000   0.601156  0.030612    0.187996\n",
    "aggregate  0.677288  0.081155  0.601156   1.000000  0.036624    0.453200\n",
    "GPT        0.110251  0.145570  0.030612   0.036624  1.000000    0.095478\n",
    "\n",
    "dim3\n",
    "225\n",
    "RAW:\n",
    "               elin      lena     oscar  aggregate       GPT  mean_human\n",
    "elin       1.000000  0.746667  0.857778   1.000000  0.746667    0.802222\n",
    "lena       0.746667  1.000000  0.808889   0.746667  0.702222    0.777778\n",
    "oscar      0.857778  0.808889  1.000000   0.857778  0.737778    0.833333\n",
    "aggregate  1.000000  0.746667  0.857778   1.000000  0.746667    0.868148\n",
    "GPT        0.746667  0.702222  0.737778   0.746667  1.000000    0.728889\n",
    "KAPPA:\n",
    "               elin      lena     oscar  aggregate       GPT  mean_human\n",
    "elin       1.000000  0.312590  0.629630   1.000000  0.468812    0.471110\n",
    "lena       0.312590  1.000000  0.354354   0.312590  0.303245    0.333472\n",
    "oscar      0.629630  0.354354  1.000000   0.629630  0.405242    0.491992\n",
    "aggregate  1.000000  0.312590  0.629630   1.000000  0.468812    0.647407\n",
    "GPT        0.468812  0.303245  0.405242   0.468812  1.000000    0.392433\n",
    "\n",
    "\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
